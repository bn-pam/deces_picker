## Projet ETL, source 1 : R√©cup√©ration de fichiers mensuel des d√©c√®s, avec beautifulsoup

### --- choix de la solution de scraping ---

| Crit√®re          | BeautifulSoup                         | Scrapy                                   | Selenium                               |
|-----------------|------------------------------------|----------------------------------------|----------------------------------------|
| **Type**        | Parsing HTML/XML                   | Framework de scraping                  | Automatisation navigateur (WebDriver) |
| **Utilisation principale** | Extraction de donn√©es simples depuis du HTML statique | Scraping avanc√©, avec gestion d‚ÄôURLs et pipelines | Interaction avec des pages dynamiques (JavaScript, formulaires, etc.) |
| **Performance** | Rapide pour du HTML statique      | Tr√®s performant pour du scraping √† grande √©chelle | Lourd et lent (d√ª √† l'ex√©cution d'un navigateur) |
| **Facilit√© d'utilisation** | Facile, syntaxe simple | Complexe mais puissant | Facile √† utiliser mais demande plus de ressources |
| **Gestion JavaScript** | ‚ùå (Ne supporte pas JS) | ‚ùå (Ne supporte pas JS nativement) | ‚úÖ (Ex√©cute le JS via le navigateur) |
| **Multithreading / Async** | ‚ùå (Non natif) | ‚úÖ (Asynchrone avec Twisted) | ‚ùå (Non adapt√© au multi-threading) |
| **Utilisation des proxies** | ‚úÖ (Mais manuel) | ‚úÖ (Facile √† int√©grer) | ‚úÖ (Mais consomme plus de ressources) |
| **Cas d‚Äôusage id√©al** | Scraping simple de pages statiques | Scraping de gros volumes, crawling de sites | Scraping de sites interactifs n√©cessitant du JS |
| **Exemples d'utilisation** | Extraire des titres d‚Äôarticles | Scraper un e-commerce en profondeur | R√©cup√©rer des infos d‚Äôune page n√©cessitant une connexion |
| **Consommation des ressources** | Faible | Moyenne | √âlev√©e (d√©marrage de navigateur) |

Pour ce projet mon choix s'est orient√© vers beautifulsoup pour sa simplicit√© et sa rapidit√©.

### --- Inventaire des fichiers ---
* ```pipeline_deces.py``` est le programme qui va lire et transformer les donn√©es en csv
* ```script_deces.py``` contient le scraper qui va r√©cup√©rer le fichier txt des d√©c√®s s'il n'existe pas d√©j√† dans le fichier ```processed_files.txt```
* ```processed_files.txt``` contient la liste des fichiers d√©j√† trait√©s
* ```deces.csv``` contient les donn√©es transform√©es en csv
* ```downloaded_files``` contient les fichiers t√©l√©charg√©s au format .txt
* 

### --- Scraping page d√©c√®s---
Le scraping contenu dans ```script_deces.py``` de ce projet se d√©roule de la mani√®re suivante :

L'url principale de t√©l√©chargement des fichiers mensuels de d√©c√®s est ouverte et la page ouverte est inspect√©e,
et pour le premier lien de fichier le programme :

> * trouve toutes les urls de fichiers des d√©c√®s gr√¢ce √† son xpath,
> * v√©rifie s'il est d√©j√† pr√©sent dans la liste des fichiers trait√©s,
> * si non, r√©cup√®re le contenu du fichier txt,
> * √©crit le contenu du fichier dans le dossier ```downloaded_files```,
> * √©crit le nom du fichier dans le fichier ```processed_files.txt```.

### --- Pipeline ---
Le pipeline contenu dans ```pipeline_deces.py``` dans ce projet se d√©roule de la mani√®re suivante :
le fichier ```deces.txt``` est lu ligne par ligne, o√π une ligne correspond aux donn√©es d'un 
d√©c√®s, et pour chacun le programme :

> * r√©cup√®re les informations (noms et pr√©noms, etc.) gr√¢ce √† des expressions r√©guli√®res,
> * nettoie les donn√©es (suppression des caract√®res sp√©ciaux, etc.),
> * transforme les donn√©es en un dictionnaire,
> * √©crit le dictionnaire dans un fichier csv. ```deces.json```,
> * √©crit le dictionnaire dans un fichier json. ```deces.csv```,

| Noms      | Prenoms           | Sexe  |Date_naiss| CP_naiss | commune_naissance | Pays_naiss|Date_deces | Lieu_deces |
|-----------|-------------------|-------|----------|----------|-------------------|------------|------|------|
| HUCHARD   | JEAN CLAUDE ANDRE | 1     | 19410420 |01053| BOURG-EN-BRESSE   |France|20240816|

### TODO :
> - ‚úÖ script de scraping du (des) fichier(s) d√©c√®s
> - ‚úÖ pipeline pour r√©cup√©rer les donn√©es brutes, et transformer les donn√©es nettoy√©es en .json
> - ‚ùå pipeline ou script pour nettoyer les donn√©es brutes et les transformer en donn√©es structur√©es
> - ‚úÖ pipeline ou script pour cr√©er une BDD {√† tester}
> - ‚ùå script d'insertion des donn√©es dans la base de donn√©es {√† tester}
> - üîÑ requirements.txt √† cr√©er ($ pip freeze > requirements.txt)
> - üîÑ README.md √† compl√©ter
> - ‚ùå tests unitaires √† √©crire
> - ‚ùå tests fonctionnels √† √©crire
> - ‚ùå tests d'int√©gration √† √©crire
> - ‚ùå tests de performance √† √©crire
> - ‚ùå am√©liorer le pipeline

### --- Sch√©ma de la table (projet) ---


| Nom du champ  | Type SQL                                                                       | D√©tails                              | 
|---------------|--------------------------------------------------------------------------------|--------------------------------------|
| id            | 	SERIAL (PostgreSQL) / INT AUTO_INCREMENT (MySQL) / IDENTITY(1,1) (SQL Server) | 	Cl√© primaire unique auto-incr√©ment√©e |
| Noms          | 	VARCHAR(255)                                                                  | 	Noms de famille                     |
| Pr√©noms       | 	VARCHAR(255)                                                                  | 	Pr√©noms                             |
| Sexe          | 	INTEGER (1)                                                                   | 	1 pour masculin 2 pour f√©minin      |
| Date_naiss    | 	Date                                                                          | 	Date de naissance                   |
| CP_naiss      | 	VARCHAR(255)                                                                  | 	Code postal de naissance            |
| commune_naiss | 	VARCHAR(255)                                                                  | 	Commune de naissance                |
| Pays_naiss    | 	VARCHAR(255)                                                                  | 	Pays de naissance                   |
| Date_deces    | 	Date                                                                          | 	Date de d√©c√®s                       |
| Lieu_deces    | 	VARCHAR(255)                                                                  | 	Lieu de d√©c√®s                       |   
| Acte_deces    | 	Date                                                                          | 	Acte de d√©c√®s si applicable         |
