## Projet ETL, source 1 : RÃ©cupÃ©ration de fichiers mensuel des dÃ©cÃ¨s, avec scrapy

### --- choix de la solution de scraping ---

| CritÃ¨re          | BeautifulSoup                         | Scrapy                                   | Selenium                               |
|-----------------|------------------------------------|----------------------------------------|----------------------------------------|
| **Type**        | Parsing HTML/XML                   | Framework de scraping                  | Automatisation navigateur (WebDriver) |
| **Utilisation principale** | Extraction de donnÃ©es simples depuis du HTML statique | Scraping avancÃ©, avec gestion dâ€™URLs et pipelines | Interaction avec des pages dynamiques (JavaScript, formulaires, etc.) |
| **Performance** | Rapide pour du HTML statique      | TrÃ¨s performant pour du scraping Ã  grande Ã©chelle | Lourd et lent (dÃ» Ã  l'exÃ©cution d'un navigateur) |
| **FacilitÃ© d'utilisation** | Facile, syntaxe simple | Complexe mais puissant | Facile Ã  utiliser mais demande plus de ressources |
| **Gestion JavaScript** | âŒ (Ne supporte pas JS) | âŒ (Ne supporte pas JS nativement) | âœ… (ExÃ©cute le JS via le navigateur) |
| **Multithreading / Async** | âŒ (Non natif) | âœ… (Asynchrone avec Twisted) | âŒ (Non adaptÃ© au multi-threading) |
| **Utilisation des proxies** | âœ… (Mais manuel) | âœ… (Facile Ã  intÃ©grer) | âœ… (Mais consomme plus de ressources) |
| **Cas dâ€™usage idÃ©al** | Scraping simple de pages statiques | Scraping de gros volumes, crawling de sites | Scraping de sites interactifs nÃ©cessitant du JS |
| **Exemples d'utilisation** | Extraire des titres dâ€™articles | Scraper un e-commerce en profondeur | RÃ©cupÃ©rer des infos dâ€™une page nÃ©cessitant une connexion |
| **Consommation des ressources** | Faible | Moyenne | Ã‰levÃ©e (dÃ©marrage de navigateur) |

Pour ce projet mon choix s'est orientÃ© vers beautifulsoup pour sa simplicitÃ© et sa rapiditÃ©.

### --- Inventaire des fichiers ---
* ```pipeline_deces.py``` est le programme qui va lire et transformer les donnÃ©es en csv
* ```script_deces.py``` contient le scraper qui va rÃ©cupÃ©rer le fichier txt des dÃ©cÃ¨s s'il n'existe pas dÃ©jÃ  dans le fichier ```processed_files.txt```
* ```processed_files.txt``` contient la liste des fichiers dÃ©jÃ  traitÃ©s
* ```deces.csv``` contient les donnÃ©es transformÃ©es en csv
* ```downloaded_files``` contient les fichiers tÃ©lÃ©chargÃ©s au format .txt
* 

### --- Scraping page dÃ©cÃ¨s---
Le scraping contenu dans ```script_deces.py``` de ce projet se dÃ©roule de la maniÃ¨re suivante :

L'url principale de tÃ©lÃ©chargement des fichiers mensuels de dÃ©cÃ¨s est ouverte et la page ouverte est inspectÃ©e,
et pour le premier lien de fichier le programme :

> * trouve toutes les urls de fichiers des dÃ©cÃ¨s grÃ¢ce Ã  son xpath,
> * vÃ©rifie s'il est dÃ©jÃ  prÃ©sent dans la liste des fichiers traitÃ©s,
> * si non, rÃ©cupÃ¨re le contenu du fichier txt,
> * Ã©crit le contenu du fichier dans le dossier ```downloaded_files```,
> * Ã©crit le nom du fichier dans le fichier ```processed_files.txt```.

### --- Pipeline ---
Le pipeline contenu dans ```pipeline_deces.py``` dans ce projet se dÃ©roule de la maniÃ¨re suivante :
le fichier ```deces.txt``` est lu ligne par ligne, oÃ¹ une ligne correspond aux donnÃ©es d'un 
dÃ©cÃ¨s, et pour chacun le programme :

> * rÃ©cupÃ¨re les informations (noms et prÃ©noms, etc.) grÃ¢ce Ã  des expressions rÃ©guliÃ¨res,
> * nettoie les donnÃ©es (suppression des caractÃ¨res spÃ©ciaux, etc.),
> * transforme les donnÃ©es en un dictionnaire,
> * Ã©crit le dictionnaire dans un fichier csv. ```deces.json```,
> * Ã©crit le dictionnaire dans un fichier json. ```deces.csv```,

| Noms      | Prenoms           | Sexe  |Date_naiss| CP_naiss | commune_naissance | Pays_naiss|Date_deces | Lieu_deces |
|-----------|-------------------|-------|----------|----------|-------------------|------------|------|------|
| HUCHARD   | JEAN CLAUDE ANDRE | 1     | 19410420 |01053| BOURG-EN-BRESSE   |France|20240816|

### TODO :
> - âœ… script de scraping du (des) fichier(s) dÃ©cÃ¨s
> - âœ… pipeline pour rÃ©cupÃ©rer les donnÃ©es brutes, et transformer les donnÃ©es nettoyÃ©es en .json
> - âŒ pipeline ou script pour nettoyer les donnÃ©es brutes et les transformer en donnÃ©es structurÃ©es
> - âœ… pipeline ou script pour crÃ©er une BDD {Ã  tester}
> - âŒ script d'insertion des donnÃ©es dans la base de donnÃ©es {Ã  tester}
> - ğŸ”„ requirements.txt Ã  crÃ©er ($ pip freeze > requirements.txt)
> - ğŸ”„ README.md Ã  complÃ©ter
> - âŒ tests unitaires Ã  Ã©crire
> - âŒ tests fonctionnels Ã  Ã©crire
> - âŒ tests d'intÃ©gration Ã  Ã©crire
> - âŒ tests de performance Ã  Ã©crire
> - âŒ amÃ©liorer le pipeline

### --- SchÃ©ma de la table (projet) ---


| Nom du champ  | Type SQL                                                                       | DÃ©tails                              | 
|---------------|--------------------------------------------------------------------------------|--------------------------------------|
| id            | 	SERIAL (PostgreSQL) / INT AUTO_INCREMENT (MySQL) / IDENTITY(1,1) (SQL Server) | 	ClÃ© primaire unique auto-incrÃ©mentÃ©e |
| Noms          | 	VARCHAR(255)                                                                  | 	Noms de famille                     |
| PrÃ©noms       | 	VARCHAR(255)                                                                  | 	PrÃ©noms                             |
| Sexe          | 	INTEGER (1)                                                                   | 	1 pour masculin 2 pour fÃ©minin      |
| Date_naiss    | 	Date                                                                          | 	Date de naissance                   |
| CP_naiss      | 	VARCHAR(255)                                                                  | 	Code postal de naissance            |
| commune_naiss | 	VARCHAR(255)                                                                  | 	Commune de naissance                |
| Pays_naiss    | 	VARCHAR(255)                                                                  | 	Pays de naissance                   |
| Date_deces    | 	Date                                                                          | 	Date de dÃ©cÃ¨s                       |
| Lieu_deces    | 	VARCHAR(255)                                                                  | 	Lieu de dÃ©cÃ¨s                       |   
| Acte_deces    | 	Date                                                                          | 	Acte de dÃ©cÃ¨s si applicable         |



Version 1.1 :
```
logging.info remplace les prints
--> pour faciliter l'analyse des erreurs grÃ¢ce aux logs
```