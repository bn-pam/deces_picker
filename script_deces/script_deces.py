import requests
from bs4 import BeautifulSoup
import os
import re
from dotenv import load_dotenv
from config import DOWNLOAD_DIR, HISTORY_FILE, CHUNK_SIZE, URL, PATTERN
import logging

load_dotenv() # Charger les variables d'environnement
logging.basicConfig(level=logging.INFO) # Configurer le logging pour afficher les messages INFO

# URL de la page contenant les fichiers
URL = URL

# Pattern regex pour reconna√Ætre les fichiers "deces-YYYY-mMM"
PATTERN = PATTERN

# Dossiers et fichiers de suivi
DOWNLOAD_DIR = DOWNLOAD_DIR # Dossier de t√©l√©chargement
HISTORY_FILE = HISTORY_FILE # Fichier d'historisation des fichiers t√©l√©charg√©s
CHUNK_SIZE = CHUNK_SIZE  # Assure que c'est un nombre entier

# Cr√©er le dossier de t√©l√©chargement s'il n'existe pas
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

def read_downloaded_files():
    """Lit les fichiers d√©j√† trait√©s et historis√©s."""
    if not os.path.exists(HISTORY_FILE): # Si le fichier n'existe pas,
        return set() # retourner un ensemble vide

    with open(HISTORY_FILE, "r", encoding="utf-8") as file: # ouvrir le fichier d'historisation en mode lecture
        return set(file.read().splitlines()) # retourner un ensemble de lignes lues

def add_to_downloaded_files(url): # Ajouter une URL au fichier de suivi
    """Ajoute une URL au fichier d'historique des fichiers t√©l√©charg√©s."""
    with open(HISTORY_FILE, "a", encoding="utf-8") as file: # ouvrir le fichier d'historisation en mode ajout
        file.write(url + "\n") # √©crire l'URL suivie d'un retour √† la ligne

def download_file(url, file_name): # T√©l√©charger un fichier et l'enregistrer localement
    """T√©l√©charge un fichier et l'enregistre localement."""
    response = requests.get(url, stream=True) # effectuer une requ√™te GET en mode streaming
    file_path = os.path.join(DOWNLOAD_DIR, file_name) # d√©finir le chemin du fichier √† t√©l√©charger

    if response.status_code == 200: # Si la requ√™te est un succ√®s
        with open(file_path, "wb") as file: # ouvrir le fichier en mode √©criture binaire
            for chunk in response.iter_content(CHUNK_SIZE): # it√©rer sur les chunks de 1024 octets
                file.write(chunk) # √©crire chaque chunk dans le fichier
        logging.info(f"‚úÖ T√©l√©charg√© : {file_name}") # afficher un message de succ√®s
    else:
        logging.info(f"‚ùå Erreur {response.status_code} pour {url}") # afficher un message d'erreur

def extract_data(): # Fonction principale pour scraper la page et t√©l√©charger les fichiers
    """Scrape la page, t√©l√©charge les nouveaux fichiers et met √† jour l'historique."""
    # Lire les fichiers d√©j√† trait√©s
    downloaded_files = read_downloaded_files()

    # T√©l√©charger et parser la page HTML
    response = requests.get(URL) # effectuer une requ√™te GET sur l'URL
    soup = BeautifulSoup(response.text, "html.parser") # parser le contenu de la r√©ponse

    # Extraire tous les liens
    for link in soup.find_all("a", href=True): # trouver tous les liens avec une URL
        file_url = link["href"] # extraire l'URL du lien
        if re.search(PATTERN, file_url):  # V√©rifier si le lien correspond au pattern
            full_url = file_url if file_url.startswith("http") else f"https://www.data.gouv.fr{file_url}" # construire l'URL compl√®te

            if full_url in downloaded_files: # V√©rifier si le fichier a d√©j√† √©t√© trait√©
                logging.info(f"üîÑ D√©j√† t√©l√©charg√© : {full_url}") # afficher un message de statut
            else:
                file_name = file_url.split("/")[-1]  # Extraire le nom du fichier
                download_file(full_url, file_name) # T√©l√©charger le fichier
                add_to_downloaded_files(full_url) # Ajouter l'URL au fichier d'historique

if __name__ == "__main__":
    extract_data() # Appeler la fonction principale
